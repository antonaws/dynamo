Common:
  model: "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
  block-size: 64
  max-model-len: 16384

Frontend:
  served_model_name: "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
  endpoint: "dynamo.Processor.chat/completions"
  port: 8000

Processor:
  router: "round-robin"
  common-configs: ["model", "block-size", "max-model-len"]
  ServiceArgs:
    workers: 1
    resources:
      cpu: "2"
      memory: "8Gi"

VllmWorker:
  enforce-eager: true
  max-num-batched-tokens: 16384
  enable-prefix-caching: true
  router: "random"
  tensor-parallel-size: 1
  ServiceArgs:
    workers: 1
    resources:
      cpu: "3"
      memory: "12Gi"
      gpu: 1
